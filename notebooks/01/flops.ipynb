{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demostration of the FLOPS calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m2_cw.flops import flops_qwen, printsf\n",
    "\n",
    "hidden_features=4864\n",
    "# vocabulary_size=151936\n",
    "vocabulary_size=13\n",
    "embedding_dimension=896\n",
    "sequence_length=512\n",
    "attention_heads=14\n",
    "transformer_layers=24\n",
    "lookup_table=True\n",
    "\n",
    "total_flops = int(1e17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have a budget of 88,000 optimiser steps at 512 context length. almost doubling the number of optimiser steps that we can take since we have reduced the size of the vocabulary to only what we need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_sizes = [vocabulary_size, 13]\n",
    "\n",
    "for vocab in vocabulary_sizes:\n",
    "    embedding = flops_embedding(embedding_dimension=embedding_dimension,\n",
    "                                        sequence_length=sequence_length,\n",
    "                                        vocabulary_size=vocab)\n",
    "    printsf(embedding, 2, prefix=f\"vocab size {vocab}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_self_atte = flops_self_atte(embedding_dimension=embedding_dimension,\n",
    "                                attention_heads=attention_heads,\n",
    "                                sequence_length=sequence_length)\n",
    "\n",
    "printsf(one_self_atte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_rms_norm = flops_rmsnorm(in_features=embedding_dimension,\n",
    "                             sequence_length=sequence_length)\n",
    "\n",
    "printsf(one_rms_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_mlp = flops_mlp(in_features=embedding_dimension,\n",
    "                    hidden_features=hidden_features,\n",
    "                    out_features=embedding_dimension,\n",
    "                    sequence_length=sequence_length)\n",
    "\n",
    "printsf(one_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_transformer_block = flops_transformer(embedding_dimension=embedding_dimension,\n",
    "                                          hidden_features=hidden_features,\n",
    "                                          sequence_length=sequence_length,\n",
    "                                          attention_heads=attention_heads)\n",
    "\n",
    "all_transformer_blocks = transformer_layers * one_transformer_block\n",
    "\n",
    "printsf(one_transformer_block)\n",
    "printsf(all_transformer_blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_sizes = [vocabulary_size, 13]\n",
    "\n",
    "for vocab in vocabulary_sizes:\n",
    "    one_lm_head = flops_linear(in_features=embedding_dimension,\n",
    "                            out_features=vocab,\n",
    "                            sequence_length=sequence_length,\n",
    "                            bias=True)\n",
    "\n",
    "    printsf(one_lm_head, 2, prefix=f\"vocab size {vocab}\\n\", add_newline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass_via_lookup_table_embedding = flops_qwen(hidden_features=hidden_features,\n",
    "                                           vocabulary_size=vocabulary_size,\n",
    "                                           embedding_dimension=embedding_dimension,\n",
    "                                           sequence_length=sequence_length,\n",
    "                                           attention_heads=attention_heads,\n",
    "                                           transformer_layers=transformer_layers,\n",
    "                                           lookup_table=True)\n",
    "\n",
    "forward_pass_via_matrix_embedding = flops_qwen(hidden_features=hidden_features,\n",
    "                                           vocabulary_size=vocabulary_size,\n",
    "                                           embedding_dimension=embedding_dimension,\n",
    "                                           sequence_length=sequence_length,\n",
    "                                           attention_heads=attention_heads,\n",
    "                                           transformer_layers=transformer_layers,\n",
    "                                           lookup_table=False)\n",
    "\n",
    "printsf(forward_pass_via_lookup_table_embedding) \n",
    "printsf(forward_pass_via_matrix_embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_lengths = [128, 512, 768]\n",
    "vocabulary_sizes = [vocabulary_size, 13]\n",
    "\n",
    "for seq_length in context_lengths:\n",
    "    for vocab_size in vocabulary_sizes:\n",
    "        forward_pass = flops_qwen(vocabulary_size=vocab_size,\n",
    "                                  sequence_length=seq_length,\n",
    "                                  lookup_table=False)\n",
    "        printsf(forward_pass, 2, prefix=f\"context {seq_length}, vocab {vocab_size}\\n\", add_newline=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Flops: 3.6e+16\n",
      "Training Flops: 6.4e+16\n",
      "59200\n"
     ]
    }
   ],
   "source": [
    "flops_training = flops_qwen(embedding_dimension=embedding_dimension,\n",
    "                   hidden_features=hidden_features,\n",
    "                   sequence_length=sequence_length,\n",
    "                   attention_heads=attention_heads,\n",
    "                   transformer_layers=transformer_layers,\n",
    "                   vocabulary_size=vocabulary_size,\n",
    "                   lookup_table=False,\n",
    "                   mode=\"training\",\n",
    "                   batch_size=1,\n",
    "                   lora_rank=1)\n",
    "\n",
    "flops_inference = flops_qwen(embedding_dimension=embedding_dimension,\n",
    "                   hidden_features=hidden_features,\n",
    "                   sequence_length=sequence_length,\n",
    "                   attention_heads=attention_heads,\n",
    "                   transformer_layers=transformer_layers,\n",
    "                   vocabulary_size=vocabulary_size,\n",
    "                   lookup_table=False,\n",
    "                   mode=\"inference\",\n",
    "                   lora_rank=1,\n",
    "                   generation_length=20 * 13)\n",
    "                   \n",
    "total_inference_flops = 0\n",
    "total_inference_flops += flops_inference * 100 * 3 # Flops for the three big inference steps at the beginning, middle, and end\n",
    "total_inference_flops += flops_inference * 10 * 10 # Flops for small inference steps in hyperparameter search\n",
    "\n",
    "total_training_flops = total_flops - total_inference_flops\n",
    "\n",
    "printsf(total_inference_flops, prefix=\"Inference Flops\", sf=2)\n",
    "printsf(total_training_flops, prefix=\"Training Flops\", sf=2)\n",
    "\n",
    "print(total_training_flops // flops_training)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2 Coursework (Python)",
   "language": "python",
   "name": "m2_coursework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
