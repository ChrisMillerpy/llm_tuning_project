{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demostration of the FLOPS calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from m2_cw.flops import FlopsConfig\n",
    "\n",
    "config = FlopsConfig()\n",
    "\n",
    "total_flops = int(1e17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15e+07\n"
     ]
    }
   ],
   "source": [
    "from m2_cw.flops import flops_embedding\n",
    "\n",
    "print(f\"{flops_embedding(config):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2e+09\n"
     ]
    }
   ],
   "source": [
    "from m2_cw.flops import flops_self_attention\n",
    "\n",
    "print(f\"{flops_self_attention(config):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.38e+06\n"
     ]
    }
   ],
   "source": [
    "from m2_cw.flops import flops_rmsnorm\n",
    "\n",
    "print(f\"{flops_rmsnorm(config.embedding_dimension, config.sequence_length):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.34e+10\n"
     ]
    }
   ],
   "source": [
    "from m2_cw.flops import flops_mlp\n",
    "\n",
    "print(f\"{flops_mlp(config):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.19e+07\n"
     ]
    }
   ],
   "source": [
    "from m2_cw.flops import flops_linear\n",
    "\n",
    "print(f\"{flops_linear(config.embedding_dimension, config.vocabulary_size, config.sequence_length):.3g}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Training Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Pass: 1.5e+12\n",
      "Inference Pass: 7.1e+13\n",
      "Forward Pass: 3.7e+11\n",
      "Inference / Forward: 191.0\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import fields\n",
    "from m2_cw.flops import flops_qwen\n",
    "\n",
    "training_config = FlopsConfig(batch_size=2)\n",
    "inference_config = FlopsConfig(\n",
    "    generation_length=20*10,\n",
    "    mode=\"inference\",\n",
    ")\n",
    "\n",
    "sf = 2\n",
    "\n",
    "training_pass = flops_qwen(training_config)\n",
    "forward_pass = training_pass / (training_config.batch_size + 2)\n",
    "inference_pass = flops_qwen(inference_config)\n",
    "\n",
    "print(f\"Training Pass: {training_pass:.{sf}g}\")\n",
    "print(f\"Inference Pass: {inference_pass:.{sf}g}\")\n",
    "print(f\"Forward Pass: {forward_pass:.{sf}g}\")\n",
    "print(f\"Inference / Forward: {inference_pass // forward_pass}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67662.0\n",
      "1412.0\n"
     ]
    }
   ],
   "source": [
    "max_training_passes = total_flops // training_pass\n",
    "max_inference_passes = total_flops // inference_pass\n",
    "\n",
    "print(f\"{max_training_passes}\")\n",
    "print(f\"{max_inference_passes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flop Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 3.89e+15\n",
      "default: 1.95e+15\n",
      "hyper1: 3.51e+15\n",
      "hyper2: 1.19e+15\n",
      "final: 3.89e+15\n",
      "Total Inference Flops: 1.44e+16, 14.43% of budget. \n",
      "Total Forecasts: 185.\n",
      "\n",
      "default: 7.39e+15\n",
      "hyper1: 4.19e+16\n",
      "hyper2: 6.18e+15\n",
      "final: 2.96e+16\n",
      "Total Train Flops: 8.5e+16, 85.03% of budget.\n",
      "Total Optimiser Steps: 57850.\n",
      "\n",
      "Total Usage: 99.46%.\n"
     ]
    }
   ],
   "source": [
    "from m2_cw.flops import InferenceConfig, TrainConfig\n",
    "\n",
    "tokens_per_time_step = 11\n",
    "num_time_steps = 20\n",
    "generation_length = tokens_per_time_step * num_time_steps\n",
    "\n",
    "full_forecasts = 50\n",
    "mid_forecasts = 25\n",
    "h1_forecasts = 5\n",
    "h2_forecasts = 5\n",
    "\n",
    "\n",
    "inference_configs = {\n",
    "    \"baseline\": [\n",
    "        {\n",
    "            \"config\": InferenceConfig(generation_length=generation_length,\n",
    "                                      lora_rank=0),\n",
    "            \"num_forecasts\": full_forecasts\n",
    "        },\n",
    "    ],\n",
    "    \"default\": [\n",
    "        {\n",
    "            \"config\": InferenceConfig(generation_length=generation_length,\n",
    "                                  lora_rank=4),\n",
    "            \"num_forecasts\": mid_forecasts\n",
    "        },\n",
    "    ],\n",
    "    \"hyper1\": [\n",
    "        {\n",
    "            \"config\": InferenceConfig(generation_length=generation_length,\n",
    "                                  lora_rank=2),\n",
    "            \"num_forecasts\": 3 * h1_forecasts,\n",
    "        },\n",
    "        {\n",
    "            \"config\": InferenceConfig(generation_length=generation_length,\n",
    "                                  lora_rank=4),\n",
    "            \"num_forecasts\": 3 * h1_forecasts,\n",
    "        },\n",
    "        {\n",
    "            \"config\": InferenceConfig(generation_length=generation_length,\n",
    "                                  lora_rank=8),\n",
    "            \"num_forecasts\": 3* h1_forecasts,\n",
    "        },\n",
    "    ],\n",
    "    \"hyper2\": [\n",
    "        {\n",
    "            \"config\": InferenceConfig(sequence_length=128,\n",
    "                                  generation_length=generation_length,\n",
    "                                  lora_rank=4),\n",
    "            \"num_forecasts\": h2_forecasts,\n",
    "        },\n",
    "        {\n",
    "            \"config\": InferenceConfig(sequence_length=512,\n",
    "                                  generation_length=generation_length,\n",
    "                                  lora_rank=4),\n",
    "            \"num_forecasts\": h2_forecasts,\n",
    "        },\n",
    "        {\n",
    "            \"config\": InferenceConfig(sequence_length=768),\n",
    "            \"num_forecasts\": h2_forecasts,\n",
    "        },\n",
    "    ],\n",
    "    \"final\": [\n",
    "        {\n",
    "            \"config\": InferenceConfig(sequence_length=512,\n",
    "                                  generation_length=generation_length,\n",
    "                                  lora_rank=4),\n",
    "            \"num_forecasts\": full_forecasts,\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "batch_size = 2\n",
    "max_steps = 20000\n",
    "mid_steps = 5000\n",
    "h1_steps = 3150\n",
    "h2_steps = 1500\n",
    "\n",
    "train_configs = {\n",
    "    \"default\": [\n",
    "        {\n",
    "            \"config\": TrainConfig(lora_rank=4,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": mid_steps\n",
    "        },\n",
    "    ],\n",
    "    \"hyper1\": [\n",
    "        {\n",
    "            \"config\": TrainConfig(lora_rank=2,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": 3 * h1_steps,\n",
    "        },\n",
    "        {\n",
    "            \"config\": TrainConfig(lora_rank=4,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": 3 * h1_steps,\n",
    "        },\n",
    "        {\n",
    "            \"config\": TrainConfig(lora_rank=8,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": 3* h1_steps,\n",
    "        },\n",
    "    ],\n",
    "    \"hyper2\": [\n",
    "        {\n",
    "            \"config\": TrainConfig(sequence_length=128,\n",
    "                                  lora_rank=4,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": h2_steps,\n",
    "        },\n",
    "        {\n",
    "            \"config\": TrainConfig(sequence_length=512,\n",
    "                                  lora_rank=4,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": h2_steps,\n",
    "        },\n",
    "        {\n",
    "            \"config\": TrainConfig(sequence_length=768,\n",
    "                                  lora_rank=4,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": h2_steps,\n",
    "        },\n",
    "    ],\n",
    "    \"final\": [\n",
    "        {\n",
    "            \"config\": TrainConfig(sequence_length=512,\n",
    "                                  lora_rank=4,\n",
    "                                  batch_size=batch_size),\n",
    "            \"num_steps\": max_steps,\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "inference_flops = {}\n",
    "inference_passes = 0\n",
    "for title, config_list in inference_configs.items():\n",
    "    flops = 0\n",
    "    for run in config_list:\n",
    "        flops += flops_qwen(run[\"config\"]) * run[\"num_forecasts\"]\n",
    "        inference_passes += run[\"num_forecasts\"]\n",
    "    \n",
    "    inference_flops[title] = flops\n",
    "\n",
    "total_inference_flops = 0\n",
    "for k, v in inference_flops.items():\n",
    "    print(f\"{k}: {v:.3g}\")\n",
    "    total_inference_flops += v\n",
    "\n",
    "print(f\"Total Inference Flops: {total_inference_flops:.3g}, {100 * total_inference_flops / total_flops:.2f}% of budget. \\nTotal Forecasts: {inference_passes}.\\n\")\n",
    "\n",
    "train_flops = {}\n",
    "opt_steps = 0\n",
    "for title, config_list in train_configs.items():\n",
    "    flops = 0\n",
    "    for run in config_list:\n",
    "        flops += flops_qwen(run[\"config\"]) * run[\"num_steps\"]\n",
    "        opt_steps += run[\"num_steps\"]\n",
    "    \n",
    "    train_flops[title] = flops\n",
    "\n",
    "total_train_flops = 0\n",
    "for k, v in train_flops.items():\n",
    "    print(f\"{k}: {v:.3g}\")\n",
    "    total_train_flops += v\n",
    "\n",
    "print(f\"Total Train Flops: {total_train_flops:.3g}, {100 * total_train_flops / total_flops:.2f}% of budget.\\nTotal Optimiser Steps: {opt_steps}.\\n\")\n",
    "\n",
    "print(f\"Total Usage: { 100 * (total_train_flops + total_inference_flops) / total_flops :.2f}%.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2 Coursework (Python)",
   "language": "python",
   "name": "m2_coursework"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
