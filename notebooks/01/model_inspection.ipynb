{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We explore the `Qwen2.5-0.5B-Instruct` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(13, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=13, bias=True)\n",
      ")\n",
      "<class 'transformers.models.qwen2.modeling_qwen2.Qwen2ForCausalLM'>\n"
     ]
    }
   ],
   "source": [
    "from m2_cw.qwen import load_qwen\n",
    "import torch\n",
    "import inspect\n",
    "\n",
    "model, tokenizer, token_map = load_qwen(small_vocabulary=True)\n",
    "\n",
    "print(model)\n",
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Summary:\n",
    "- Step 0: Notation\n",
    "    - $\\mathcal{V}$ = Vocabulary, here $|\\mathcal{V}| = 151,936$. All possible tokens that our model can recognise. \n",
    "        - eg: \"1\", \"cat\", \"ham\", \"?\", ...\n",
    "    - $D$ = Dimension of the embedding space. Each token in $\\mathcal{V}$ is mapped to a learned embedding vector in $\\mathbb{R}^D$.\n",
    "        - eg: \"cat\" -> $(0.321, 0.341, 2.3, -0.5, ...) \\in \\mathcal{R}^D$.\n",
    "    - $N$ = Length of the input sequence (in tokens). \n",
    "        - eg:  \"The cat is fat.\" -> [\"The \", \"cat \", \"is \", \"fat\", \".\"], $N=5$.\n",
    "- Step 1: Embedding\n",
    "    - Sequence of tokens -> Sparce One Hot encoded matrix -> embedding matrix\n",
    "        - $N$ tokens -> $N \\times |\\mathcal{V}|$ Matrix -> $N \\times D$ Matrix of embedding vectors.\n",
    "- Step 2: 24 Decoder layers\n",
    "    - Theory:\n",
    "        - input saved as residual 1.\n",
    "            - input layernorm RMSNorm.\n",
    "            - self attention.\n",
    "                - 14 Heads, with rotary positional encoding on keys and values\n",
    "        - residual 1 added. Saved as residual 2\n",
    "            - post attention layernorm RMSNorm.\n",
    "            - MLP\n",
    "        - residual 2 added.\n",
    "- Step 3: Language Model Head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.sparse.Embedding'>\n",
      "torch.Size([3])\n",
      "torch.Size([3, 896])\n"
     ]
    }
   ],
   "source": [
    "embedding = model.model.embed_tokens\n",
    "\n",
    "tokens = torch.tensor([151000, 4312, 121])\n",
    "embeddings = embedding(tokens)\n",
    "\n",
    "print(type(embedding))\n",
    "\n",
    "print(tokens.shape)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Self-Attention Layer x 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/chrismiller/venvs/m2_venvs/m2_coursework/lib/python3.13/site-packages/transformers/models/qwen2/modeling_qwen2.py\n"
     ]
    }
   ],
   "source": [
    "decoder_layer = model.model.layers[0]\n",
    "self_attn = decoder_layer.self_attn\n",
    "q = self_attn.q_proj\n",
    "k = self_attn.k_proj\n",
    "v = self_attn.v_proj\n",
    "\n",
    "print(inspect.getabsfile(self_attn.forward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.Size([4, 2, 2])\n",
      "tensor([[ 1,  2],\n",
      "        [ 5,  6],\n",
      "        [ 9, 10],\n",
      "        [13, 14]])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 896\n",
    "\n",
    "test_q = torch.tensor([[1, 2, 3, 4],\n",
    "                      [5, 6, 7, 8],\n",
    "                      [9, 10, 11, 12],\n",
    "                      [13, 14, 15, 16]])\n",
    "print(test_q.shape) # >>> (4, 4) = (Embedding Dimension, Number of Tokens)\n",
    "new_shape = (4, -1, 2)\n",
    "new_q = test_q.view(new_shape)\n",
    "print(new_q.shape)\n",
    "print(new_q[:, 0, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear(in_features=896, out_features=151936, bias=True)\n"
     ]
    }
   ],
   "source": [
    "head = model.lm_head\n",
    "print(head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen2ForCausalLM(\n",
      "  (model): Qwen2Model(\n",
      "    (embed_tokens): Embedding(151936, 896)\n",
      "    (layers): ModuleList(\n",
      "      (0-23): 24 x Qwen2DecoderLayer(\n",
      "        (self_attn): Qwen2Attention(\n",
      "          (q_proj): Linear(in_features=896, out_features=896, bias=True)\n",
      "          (k_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (v_proj): Linear(in_features=896, out_features=128, bias=True)\n",
      "          (o_proj): Linear(in_features=896, out_features=896, bias=False)\n",
      "        )\n",
      "        (mlp): Qwen2MLP(\n",
      "          (gate_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (up_proj): Linear(in_features=896, out_features=4864, bias=False)\n",
      "          (down_proj): Linear(in_features=4864, out_features=896, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "        (post_attention_layernorm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Qwen2RMSNorm((896,), eps=1e-06)\n",
      "    (rotary_emb): Qwen2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=896, out_features=151936, bias=True)\n",
      ")\n",
      "/Users/chrismiller/venvs/m2_venvs/m2_coursework/lib/python3.13/site-packages/torch/utils/_contextlib.py\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "\n",
    "print(inspect.getabsfile(model.generate))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "M2 Coursework (Python)",
   "language": "python",
   "name": "m2_coursework"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
